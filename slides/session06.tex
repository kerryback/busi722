\documentclass[aspectratio=169]{beamer}
\usetheme{metropolis}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\input{busi722-style}

\title{Trading Costs, Risk Forecasts}
\subtitle{BUSI 722: Data-Driven Finance II}
\author{Kerry Back}
\institute{}
\date{}

\begin{document}

\maketitle

%%% PART 1: TRANSACTION COSTS %%%

\begin{frame}[c]
\centering
\Huge Transaction Costs
\end{frame}

\begin{frame}{Categories of Transaction Costs}
\begin{itemize}
\item \textbf{Commissions and bid-ask spread:} broker fees are near zero today, but the bid-ask spread (half-spread per trade) remains a meaningful cost, especially for illiquid stocks.
\item \textbf{Market impact:} large orders move the price against you.  This is typically the dominant cost for institutional strategies.
\item \textbf{Opportunity cost:} trades you cancel or delay because the market moved before you could execute.
\end{itemize}
\end{frame}

\begin{frame}{Market Impact}
Market impact grows with trade size relative to the stock's liquidity:
\[
\text{impact} \;\approx\; c \times \sqrt{\frac{\text{shares traded}}{\text{average daily volume}}}
\]

\begin{itemize}
\item The square-root law is a widely used approximation.
\item Impact is larger for small, illiquid stocks.
\item This creates a \alert{capacity constraint}: strategies that trade heavily in small stocks face rapidly increasing costs as the fund grows.
\end{itemize}
\end{frame}

\begin{frame}{Estimating Transaction Costs}
A simple model for backtesting:
\[
\text{cost}_t = \sum_{i=1}^{n} c_i \times |w_{i,t} - w_{i,t-1}|
\]

\begin{itemize}
\item $|w_{i,t} - w_{i,t-1}|$ = the absolute change in weight (turnover) for stock $i$.
\item $c_i$ = estimated one-way cost for stock $i$ (e.g., half the bid-ask spread plus estimated market impact).
\item Common simplification: use a flat cost per unit of turnover (e.g., 5--20 basis points one-way).
\end{itemize}
\end{frame}

%%% SHORT-SELLING COSTS %%%

\begin{frame}[c]
\centering
\Huge Short-Selling Costs
\end{frame}

\begin{frame}{What Does Shorting Cost?}
To short a stock, you borrow shares and sell them.  This incurs:

\begin{itemize}
\item \textbf{Borrow fee:} an annualized fee paid to the lender (typically 0.3--1\% for easy-to-borrow stocks, but can be 10--50\%+ for hard-to-borrow names).
\item \textbf{Margin and recall risk:} you must post collateral, and the lender can recall shares at any time, forcing you to close the position.
\item \textbf{Short-sale constraints:} some stocks are simply unavailable to borrow.
\end{itemize}
\end{frame}

\begin{frame}{Implications for Long-Short Strategies}
\begin{itemize}
\item ML models often find the \alert{short side is more profitable} than the long side --- but short-selling costs disproportionately affect small, illiquid stocks where signals are strongest.
\item A strategy that looks great in a frictionless backtest may be mediocre or unprofitable after accounting for borrow fees.
\item \alert{Always evaluate long-only and long-short separately} to see where the value is coming from.
\end{itemize}
\end{frame}

%%% NET-OF-COST EVALUATION %%%

\begin{frame}[c]
\centering
\Huge Net-of-Cost Evaluation
\end{frame}

\begin{frame}{Net-of-Cost Portfolio Returns}
The net portfolio return in month $t$:
\[
r_{p,t}^{\text{net}} = r_{p,t}^{\text{gross}} - \text{transaction costs}_t - \text{borrow costs}_t
\]

\begin{itemize}
\item Recompute Sharpe ratios, CAPM alphas, and information ratios using $r_{p,t}^{\text{net}}$.
\item This is the \alert{real test} of a strategy.  Many academic strategies fail it.
\end{itemize}

\vspace{0.3cm}
\textbf{Avramov, Cheng, and Metzker} (\textit{Management Science}, 2023):
\begin{itemize}
\item ML profitability concentrates in \alert{hard-to-trade} stocks (microcaps, distressed firms).
\item Excluding these stocks or adding realistic trading costs significantly reduces profitability.
\end{itemize}
\end{frame}

%%% REBALANCING AND WEIGHTING %%%

\begin{frame}[c]
\centering
\Huge Rebalancing \& Weighting
\end{frame}

\begin{frame}{Rebalancing Frequency}
How often should we retrain the model and rebalance the portfolio?
\vspace{0.3cm}

\begin{center}
\begin{tabular}{lll}
\textbf{Frequency} & \textbf{Pro} & \textbf{Con} \\
\hline
Monthly & tracks signal closely & high turnover, high cost \\
Quarterly & moderate turnover & signal may be stale \\
Annually & low turnover & very stale signal \\
\end{tabular}
\end{center}

\vspace{0.3cm}
\begin{itemize}
\item The optimal frequency depends on the \alert{decay rate of the signal} versus the \alert{cost of trading}.
\item Momentum decays quickly (monthly rebalancing is typical); value signals are slow-moving (quarterly may suffice).
\item A hybrid approach: retrain monthly but only trade when the change in target weight exceeds a threshold (\alert{buffer rules}).
\end{itemize}
\end{frame}

\begin{frame}{Turnover}
\textbf{Turnover} measures the fraction of the portfolio that changes each period:
\[
\text{turnover}_t = \sum_{i=1}^{n} |w_{i,t} - w_{i,t-1}^{+}|
\]
where $w_{i,t-1}^{+}$ is the weight of stock $i$ at the end of the previous month (after returns).

\vspace{0.3cm}
\begin{itemize}
\item A turnover of 1.0 means the entire portfolio is replaced.
\item A long-short decile portfolio rebalanced monthly can have turnover $> 1.0$.
\item Smooth weight functions (linear, power) typically generate \alert{lower turnover} than sort-based step functions, because small changes in predicted rank produce small changes in weight.
\end{itemize}
\end{frame}

\begin{frame}{Value Weighting to Reduce Small-Cap Exposure}
Equal-weighted portfolios tilt heavily toward small stocks, which are:
\begin{itemize}
\item harder and more expensive to trade,
\item where many ML signals are strongest (possibly spuriously),
\item not investable at scale.
\end{itemize}

\vspace{0.3cm}
\textbf{Solutions:}
\begin{itemize}
\item \textbf{Value-weight within groups:} sort into deciles by predicted rank, then value-weight (by market cap) within each decile.
\item \textbf{Score-tilted market-cap weights} (from Session 5): $w_i \propto \text{mcap}_i \times g(u_i)$.  Stays close to the market-cap benchmark.
\item \textbf{Market-cap filters:} exclude micro-caps or nano-caps entirely.  Evaluate only on stocks you could actually trade.
\end{itemize}
\end{frame}

%%% PORTFOLIO CONSTRUCTION ALTERNATIVES %%%

\begin{frame}[c]
\centering
\Huge Alternative Portfolio Construction Methods
\end{frame}

\begin{frame}{Mean-Variance Optimization}
\textbf{Markowitz (1952):} choose weights $\mathbf{w}$ to maximize
\[
\mathbf{w}'\boldsymbol{\mu} - \frac{\gamma}{2}\,\mathbf{w}'\boldsymbol{\Sigma}\mathbf{w}
\]
The \alert{tangency portfolio} maximizes the Sharpe ratio:
\[
\mathbf{w}^* \;\propto\; \boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}
\]

\vspace{0.3cm}
\textbf{Pros:}
\begin{itemize}
\item Theoretically optimal given correct inputs.
\item Balances return and risk.
\end{itemize}
\textbf{Cons:}
\begin{itemize}
\item Extremely sensitive to estimation errors in $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$.
\item Produces extreme, concentrated weights.
\item With noisy return forecasts, often \alert{worse} than $1/N$ out of sample.
\end{itemize}
\end{frame}

\begin{frame}{Minimum-Variance Portfolio}
Set $\boldsymbol{\mu} = \mathbf{0}$ and solve only for the lowest-risk portfolio:
\[
\mathbf{w}^* = \arg\min_{\mathbf{w}} \;\mathbf{w}'\boldsymbol{\Sigma}\mathbf{w} \quad \text{s.t.} \quad \mathbf{w}'\mathbf{1} = 1
\]

\vspace{0.3cm}
\textbf{Pros:}
\begin{itemize}
\item Requires \alert{only the covariance matrix} --- no return forecasts needed.
\item Covariances are more stable and easier to estimate than expected returns.
\item Historically competitive Sharpe ratios despite ignoring returns.
\end{itemize}
\textbf{Cons:}
\begin{itemize}
\item Concentrates in low-volatility stocks (can be extreme without constraints).
\item Still sensitive to estimation error in $\boldsymbol{\Sigma}$ --- use shrinkage estimators (Ledoit-Wolf).
\item Ignores return predictions entirely.
\end{itemize}
\end{frame}

\begin{frame}{Risk Parity}
Equalize each asset's \alert{contribution to total portfolio risk}:
\[
w_i \times (\boldsymbol{\Sigma}\mathbf{w})_i = \frac{1}{n}\,\mathbf{w}'\boldsymbol{\Sigma}\mathbf{w} \quad \text{for all } i
\]

A simple approximation: $w_i \propto 1/\sigma_i$ (inverse-volatility weighting).

\vspace{0.3cm}
\textbf{Pros:}
\begin{itemize}
\item More diversified than minimum variance --- no single asset dominates risk.
\item No return forecasts needed.
\item Robust to estimation error; widely used in practice (e.g., Bridgewater's All Weather).
\end{itemize}
\textbf{Cons:}
\begin{itemize}
\item Ignores correlations in the simple version (inverse-volatility).
\item Does not optimize for return.
\item Often requires leverage to achieve competitive expected returns.
\end{itemize}
\end{frame}

\begin{frame}{Comparing Portfolio Construction Methods}
\small
\begin{center}
\begin{tabular}{lccc}
& \textbf{Tangency} & \textbf{Min-Variance} & \textbf{Risk Parity} \\
\hline
Uses return forecasts & yes & no & no \\
Uses covariance & yes & yes & yes (or vol only) \\
Diversification & low & low & high \\
Estimation sensitivity & very high & moderate & low \\
Out-of-sample Sharpe & often poor & competitive & competitive \\
\end{tabular}
\end{center}

\vspace{0.3cm}
\begin{itemize}
\item \textbf{If you trust your return forecasts:} tangency portfolio, but regularize heavily (shrinkage, constraints).
\item \textbf{If you don't trust return forecasts:} minimum variance or risk parity.
\item \textbf{Hybrid:} use predicted ranks for stock selection (which stocks to hold), and risk-based methods for position sizing (how much of each).
\end{itemize}
\end{frame}

%%% PREDICTING RISK %%%

\begin{frame}[c]
\centering
\Huge Covariance Estimation
\end{frame}

\begin{frame}{Why Covariance Estimation Is Hard}
All risk-based methods require a covariance matrix $\boldsymbol{\Sigma}$.

\vspace{0.3cm}
\begin{itemize}
\item With $n$ stocks, $\boldsymbol{\Sigma}$ has $n(n+1)/2$ unique entries.  For 500 stocks, that is \alert{125,250 parameters} to estimate --- far more than the number of monthly observations.
\item The sample covariance \alert{overfits}: its eigenvalues are too spread out, and some may be zero or negative.
\item Portfolios optimized with a noisy $\boldsymbol{\Sigma}$ take extreme, unstable positions.
\end{itemize}
\end{frame}

\begin{frame}{Ledoit-Wolf Shrinkage: The Idea}
\textbf{Insight:} the sample covariance is \alert{unbiased but high-variance}.  A structured target is \alert{biased but low-variance}.  Blending them reduces mean-squared error.

\[
\boldsymbol{\Sigma}_{\text{shrunk}} = \delta \,\mathbf{F} + (1 - \delta)\,\mathbf{S}
\]

\begin{itemize}
\item $\mathbf{S}$ = sample covariance matrix (complex, noisy)
\item $\mathbf{F}$ = \textbf{shrinkage target} --- a simple, structured matrix
\item $\delta \in [0,1]$ = \textbf{shrinkage intensity} --- how much to shrink toward the target
\end{itemize}

\vspace{0.3cm}
This is the \alert{bias-variance tradeoff} applied to covariance estimation:
\begin{itemize}
\item $\delta = 0$: pure sample covariance (low bias, high variance)
\item $\delta = 1$: pure target (high bias, low variance)
\item $\delta^*$: optimal blend that minimizes estimation error
\end{itemize}
\end{frame}

\begin{frame}{Choosing the Shrinkage Target}
Common choices for the target $\mathbf{F}$:

\vspace{0.3cm}
\begin{itemize}
\item \textbf{Scaled identity:} $\mathbf{F} = \bar{\sigma}^2 \,\mathbf{I}$, where $\bar{\sigma}^2$ is the average diagonal of $\mathbf{S}$.  Assumes all stocks have the same variance and zero correlation.  Simple but aggressive.
\item \textbf{Single-factor model:} $\mathbf{F} = \hat{\beta}\hat{\beta}'\sigma_m^2 + \mathbf{D}$, where $\hat{\beta}$ are market betas.  Preserves the market factor structure. This is the default in \texttt{sklearn.covariance.LedoitWolf}.
\item \textbf{Constant-correlation:} all pairwise correlations equal the average sample correlation.  Preserves individual variances.
\end{itemize}

\vspace{0.3cm}
Ledoit and Wolf (2004) derive a \alert{closed-form formula} for the optimal $\delta^*$ that minimizes the expected squared Frobenius norm of the estimation error.
\end{frame}

\begin{frame}{Effect of Shrinkage}
\begin{itemize}
\item \textbf{Eigenvalues compress:} extreme eigenvalues of $\mathbf{S}$ are pulled toward the mean, and spurious correlations moderate.  The matrix becomes better-conditioned.
\item \textbf{Portfolio weights stabilize:} minimum-variance portfolios computed with $\boldsymbol{\Sigma}_{\text{shrunk}}$ have less extreme, more diversified weights.
\item \textbf{Out-of-sample risk improves:} the shrunk covariance produces portfolios with \alert{lower realized volatility} than the sample covariance.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Ledoit-Wolf in Python}
\texttt{scikit-learn} provides a one-line implementation:

\vspace{0.3cm}
\begin{shadedbox}
\small
\begin{verbatim}
from sklearn.covariance import LedoitWolf

returns = ...  # T x n array of stock returns
lw = LedoitWolf().fit(returns)

cov_shrunk = lw.covariance_    # shrunk covariance matrix
delta = lw.shrinkage_          # optimal shrinkage intensity
\end{verbatim}
\end{shadedbox}

\vspace{0.3cm}
\begin{itemize}
\item The \texttt{shrinkage\_} attribute reports the optimal $\delta^*$ --- typically 0.5--0.9 for monthly stock returns, meaning heavy shrinkage is optimal.
\item Use \texttt{cov\_shrunk} anywhere you would use a sample covariance: minimum-variance portfolio, risk parity, mean-variance optimization.
\end{itemize}
\end{frame}

\begin{frame}{Other Approaches to Covariance Estimation}
\begin{itemize}
\item \textbf{Factor models} (Barra-style): $\boldsymbol{\Sigma} = \mathbf{B}\boldsymbol{\Lambda}\mathbf{B}' + \mathbf{D}$.  Reduces dimensionality by modeling common factors.  Industry standard for large-scale risk models.
\item \textbf{GARCH / DCC:} capture time-varying volatility and correlations.  Useful when risk regimes change.
\item \textbf{Exponential weighting:} weight recent observations more heavily.  Simple, effective, and captures regime changes.
\end{itemize}

\vspace{0.3cm}
\textbf{Practical advice:} Ledoit-Wolf shrinkage is the easiest win.  It requires no modeling decisions beyond choosing a target, and the improvement over the sample covariance is reliable.  Start here, then consider factor models if you need more structure.
\end{frame}

%%% DRAWDOWNS %%%

\begin{frame}[c]
\centering
\Huge Drawdowns
\end{frame}

\begin{frame}{What Is a Drawdown?}
A \alert{drawdown} is the decline from a portfolio's peak value to a subsequent trough:
\[
\text{drawdown}_t = \frac{V_t - V_{\max,t}}{V_{\max,t}}
\]
where $V_{\max,t} = \max_{s \le t} V_s$ is the running maximum of portfolio value.

\vspace{0.3cm}
\begin{itemize}
\item Always negative (or zero at a new high).
\item The \textbf{maximum drawdown} is the largest peak-to-trough decline over the sample.
\item Drawdown \alert{duration} measures how long it takes to recover to the previous peak.
\end{itemize}
\end{frame}

\begin{frame}{Why Drawdowns Matter}
\begin{itemize}
\item The Sharpe ratio treats upside and downside volatility equally.  Investors don't.
\item A strategy with a high Sharpe ratio but a 60\% maximum drawdown is psychologically and practically devastating --- investors redeem, managers get fired.
\item Drawdowns capture \alert{path-dependent risk} that summary statistics miss.
\end{itemize}

\vspace{0.3cm}
\textbf{Reporting:}
\begin{itemize}
\item Plot the \alert{underwater chart}: drawdown over time.  Shows how often and how deeply the portfolio falls below its peak.
\item Report maximum drawdown and maximum drawdown duration alongside Sharpe ratios and alphas.
\end{itemize}
\end{frame}

\begin{frame}{Drawdowns and Strategy Evaluation}
\begin{itemize}
\item Compare drawdowns across different weight functions, rebalancing frequencies, and cost assumptions.
\item A strategy that looks great on Sharpe ratio but has deeper drawdowns than a simpler approach may not be worth the complexity.
\item Long-short strategies can have severe drawdowns during short squeezes; value-weighted portfolios typically have \alert{shallower drawdowns} (less small-cap exposure).
\end{itemize}

\vspace{0.3cm}
\textbf{Key takeaway:} always look at the full return path, not just summary statistics.  The underwater chart is one of the most important diagnostic plots for any quantitative strategy.
\end{frame}

\end{document}
