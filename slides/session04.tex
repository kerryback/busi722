\documentclass[aspectratio=169,t]{beamer}
\usetheme{metropolis}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\input{busi722-style}

\title{Rolling Windows for Train/Test}
\subtitle{BUSI 722: Data-Driven Finance II}
\author{Kerry Back}
\institute{}
\date{}

\begin{document}

\maketitle

%%% PART 1: TRAIN-TEST SPLIT %%%

\begin{frame}[c]
\centering
\Huge Train on the Past, Test on the Future
\end{frame}

\begin{frame}{Why Not a Random Split?}
In a standard ML course, you randomly split data into training and test sets.
\vspace{0.3cm}

\alert{If we randomly assign months, the training set will contain future data --- the model sees the future before predicting it.}

\vspace{0.3cm}
\begin{itemize}
\item Financial data has a natural \alert{time ordering} that must be respected.
\item The model should never train on data that comes after the prediction date.
\item This is the same look-ahead bias we avoided when constructing features.
\end{itemize}
\end{frame}

\begin{frame}{Time-Series Split}
Split the data at a fixed date: everything before is training, everything after is testing.

\vspace{0.3cm}
\begin{center}
\begin{tabular}{ccc}
$\underbrace{\text{2011} \quad \cdots \quad \text{2019}}_{\text{Training}}$ & $\Big|$ & $\underbrace{\text{2020} \quad \cdots \quad \text{2025}}_{\text{Testing}}$ \\
\end{tabular}
\end{center}

\vspace{0.3cm}
\begin{itemize}
\item Train the model on all stock-months through 2019.
\item Generate predictions for 2020--2025.
\item Evaluate: do the predictions correctly rank stocks?
\end{itemize}

\vspace{0.15cm}
\textbf{Limitation:} a single split gives only one evaluation.  The result depends heavily on where you cut.
\end{frame}

\begin{frame}{A Simple Implementation}
\alert{Prompt:} ``Read the merged data.  Standardize features cross-sectionally each month. Use months through 2019 as training and 2020 onward as test.  Fit a LightGBM model and predict return ranks in the test set.''

\vspace{0.3cm}
\begin{itemize}
\item This is a reasonable starting point but has drawbacks.
\item The model is trained once and never updated --- by 2025, the weights are six years stale.
\item Markets change and relationships evolve, so the model should be \alert{retrained} as new data arrives.
\end{itemize}
\end{frame}

%%% PART 2: MOVING WINDOWS %%%

\begin{frame}[c]
\centering
\Huge Moving Windows
\end{frame}

\begin{frame}{The Idea}
Instead of training once, \alert{retrain the model each month} (or each quarter) using data up to that point.  Predict only the next period.

\vspace{0.3cm}
\begin{enumerate}
\item Train on months 1 through $t-1$; predict month $t$ and record the predictions.
\item Move forward: train on months 1 through $t$, predict month $t+1$.
\item Repeat until the end of the sample.
\end{enumerate}

\vspace{0.3cm}
This produces a time series of \alert{out-of-sample predictions}, each made without any future information.
\end{frame}

\begin{frame}{Expanding vs.\ Rolling Windows}
\begin{itemize}
\item \textbf{Expanding window:} the training set always starts at the beginning and grows over time.  More data $=$ more stable estimates.
\item \textbf{Rolling window:} the training set is a fixed-length window (e.g., 5 years) that slides forward.  Adapts faster to changing markets.
\end{itemize}

\vspace{0.3cm}
\begin{center}
\begin{tabular}{lll}
\textbf{Month} & \textbf{Expanding} & \textbf{Rolling (5 yr)} \\
\hline
Predict 2020-01 & Train 2011--2019 & Train 2015--2019 \\
Predict 2021-01 & Train 2011--2020 & Train 2016--2020 \\
Predict 2022-01 & Train 2011--2021 & Train 2017--2021 \\
\end{tabular}
\end{center}
\end{frame}

\begin{frame}{Predicting One Period Ahead}
\textbf{Key principle:} at each step, we predict \alert{only the next month}.

\vspace{0.3cm}
\begin{itemize}
\item We are not forecasting 6 or 12 months ahead.
\item Each month, we rank stocks based on the model's prediction, then observe actual returns and record results.
\item This simulates what we would do in real time: retrain, predict, trade, repeat.
\end{itemize}

\vspace{0.3cm}
The collection of one-step-ahead predictions across all months forms the \alert{backtest}.
\end{frame}

\begin{frame}[fragile]{Moving Window in Practice}
\alert{Prompt:} ``Using an expanding window starting with 5 years of training data, retrain a LightGBM model each month and predict return ranks one month ahead.  Collect all out-of-sample predictions.''

\vspace{0.3cm}
\begin{shadedbox}
\small
\begin{verbatim}
  months = sorted(df['month'].unique())
  predictions = []
  for t in range(60, len(months)):
      train = df[df['month'] < months[t]]
      test  = df[df['month'] == months[t]]
      model.fit(train[features], train[target])
      pred  = model.predict(test[features])
      predictions.append(...)
\end{verbatim}
\end{shadedbox}
\end{frame}

%%% PART 3: SPEARMAN RANK CORRELATION %%%

\begin{frame}[c]
\centering
\Huge Spearman Rank Correlation
\end{frame}

\begin{frame}{Measuring Prediction Quality}
We predict return \alert{ranks}, not dollar returns.  So the question is: do our predicted ranks agree with the actual ranks?

\vspace{0.3cm}
\begin{itemize}
\item We don't need the predicted values to be accurate in magnitude.
\item We need the \alert{ordering} to be right: stocks we predict to be at the top should actually outperform.
\item This calls for a \alert{rank-based} metric.
\end{itemize}
\end{frame}

\begin{frame}{Spearman Rank Correlation}
The \alert{Spearman correlation} measures the agreement between two rankings.

\vspace{0.15cm}
\begin{itemize}
\item Rank the predicted values 1 through $n$.
\item Rank the actual returns 1 through $n$.
\item Compute the Pearson correlation of the two rank vectors.
\end{itemize}

\vspace{0.15cm}
\[
\rho_S = \text{corr}\bigl(\text{rank}(\hat y), \;\text{rank}(y)\bigr)
\]

\begin{itemize}
\item $\rho_S = 1$: perfect agreement in rankings.
\item $\rho_S = 0$: predictions are no better than random.
\item $\rho_S < 0$: predictions are inversely related to actual outcomes.
\end{itemize}
\end{frame}

\begin{frame}{Why Spearman?}
\begin{itemize}
\item Directly measures what we care about: can the model \alert{rank stocks correctly}?
\item Consistent with using ranked returns as the target.
\item Insensitive to outliers and does not require forming portfolios or choosing portfolio weights.
\end{itemize}

\vspace{0.3cm}
We will use Spearman as the \alert{cross-validation metric} for selecting hyperparameters: the hyperparameter setting that produces the highest Spearman on validation data is the one we choose.
\end{frame}

%%% PART 4: CROSS-VALIDATION IN THE MOVING WINDOW %%%

\begin{frame}[c]
\centering
\Huge Cross-Validation in the Moving Window
\end{frame}

\begin{frame}{The Hyperparameter Problem}
Every model has \alert{hyperparameters} that must be set before training:

\begin{itemize}
\item \textbf{LightGBM:} number of trees, learning rate, max depth, min samples per leaf
\item \textbf{Ridge:} regularization strength $\lambda$
\item \textbf{Neural net / RFF:} layers, hidden units, dropout, number of features $D$, bandwidth $\gamma$
\end{itemize}

\vspace{0.3cm}
How do we choose them without using future data?
\end{frame}

\begin{frame}{Cross-Validation Inside the Training Window}
At each step of the moving window, \alert{before} training the final model:

\vspace{0.3cm}
\begin{enumerate}
\item Split the training window into $K$ time-ordered folds.
\item For each candidate hyperparameter setting, train on folds before $k$, predict fold $k$, and average Spearman across folds.
\item Select the best hyperparameters and retrain on the full training window.
\end{enumerate}
\end{frame}

\begin{frame}{Time-Series Cross-Validation}
Standard $K$-fold CV assigns folds randomly.  For time series, folds must respect the time order.

\vspace{0.15cm}
\textbf{Example with 5 folds} (training window = 2011--2019):

\begin{center}
\small
\begin{tabular}{lll}
\textbf{Fold} & \textbf{Train} & \textbf{Validate} \\
\hline
1 & 2011--2014 & 2015 \\
2 & 2011--2015 & 2016 \\
3 & 2011--2016 & 2017 \\
4 & 2011--2017 & 2018 \\
5 & 2011--2018 & 2019 \\
\end{tabular}
\end{center}

\vspace{0.3cm}
Each validation fold comes \alert{after} its training fold --- no look-ahead bias.
\end{frame}

\begin{frame}{Spearman in Each Validation Fold}
In each validation fold, we have predictions and actual returns for all stocks in that fold's months.

\vspace{0.3cm}
\begin{itemize}
\item Compute the Spearman correlation between predicted and actual return ranks \alert{within each month} of the fold.
\item Average the monthly Spearman correlations within the fold.
\item Then average across all $K$ folds.
\end{itemize}

\vspace{0.3cm}
This gives a single number summarizing how well a given hyperparameter setting ranks stocks on held-out future data --- exactly what we need.
\end{frame}

\begin{frame}{The Complete Loop}
\begin{enumerate}
\item For each prediction month $t$: define the training window, run time-series CV to select hyperparameters, retrain on the full window, and predict month $t$.
\item Collect all one-step-ahead predictions.
\end{enumerate}

\vspace{0.3cm}
We now have out-of-sample predicted ranks for every stock in every month of the test period.  In subsequent sessions, we will use these predictions to form and evaluate portfolios.
\end{frame}

\begin{frame}{Computational Cost}
This loop is expensive: at each step, we run $K$-fold CV (training the model $K$ times per hyperparameter setting) and then train once more on the full window.

\vspace{0.3cm}
\textbf{Practical shortcuts:}
\begin{itemize}
\item Retrain quarterly instead of monthly (predict 3 months with the same model).
\item Use a coarse hyperparameter grid, or fix hyperparameters after an initial CV and only retrain weights going forward.
\item Use fast models (LightGBM, ridge) for the full loop; reserve slow models (neural nets) for fewer retraining dates.
\end{itemize}
\end{frame}

\end{document}
