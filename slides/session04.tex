\documentclass[aspectratio=169]{beamer}
\usetheme{metropolis}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\input{busi722-style}

\title{Rolling Windows for Train/Test}
\subtitle{BUSI 722: Data-Driven Finance II}
\author{Kerry Back}
\institute{}
\date{}

\begin{document}

\maketitle

%%% PART 1: TRAIN-TEST SPLIT %%%

\begin{frame}[c]
\centering
\Huge Train on the Past, Test on the Future
\end{frame}

\begin{frame}{Why Not a Random Split?}
In a standard ML course, you randomly split data into training and test sets.
\vspace{0.3cm}

\begin{shadedbox}[title=The Problem with Financial Data]
If we randomly assign months to train and test, the training set will contain \textbf{future} data relative to some test observations.  The model sees the future before predicting it.
\end{shadedbox}

\vspace{0.3cm}
\begin{baritemize}
\item Financial data has a natural \alert{time ordering} that must be respected.
\item The model should never train on data that comes after the prediction date.
\item This is the same look-ahead bias we avoided when constructing features.
\end{baritemize}
\end{frame}

\begin{frame}{Time-Series Split}
Split the data at a fixed date: everything before is training, everything after is testing.

\vspace{0.3cm}
\begin{center}
\begin{tabular}{ccc}
$\underbrace{\text{2011} \quad \cdots \quad \text{2019}}_{\text{Training}}$ & $\Big|$ & $\underbrace{\text{2020} \quad \cdots \quad \text{2025}}_{\text{Testing}}$ \\
\end{tabular}
\end{center}

\vspace{0.3cm}
\begin{baritemize}
\item Train the model on all stock-months through 2019.
\item Generate predictions for 2020--2025.
\item Evaluate: do the predictions correctly rank stocks?
\end{baritemize}

\vspace{0.3cm}
\textbf{Limitation:} a single split gives only one evaluation.  The result depends heavily on where you cut.
\end{frame}

\begin{frame}{A Simple Implementation}
\alert{Prompt:} ``Read the merged data.  Standardize features cross-sectionally each month. Use months through 2019 as training and 2020 onward as test.  Fit a LightGBM model and predict return ranks in the test set.''

\vspace{0.3cm}
\begin{baritemize}
\item This is a reasonable starting point but has drawbacks.
\item The model is trained once and never updated.
\item By 2025, the model is using weights learned from data ending in 2019 --- six years stale.
\item Markets change.  Relationships between features and returns evolve.  The model should be \alert{retrained} as new data arrives.
\end{baritemize}
\end{frame}

%%% PART 2: MOVING WINDOWS %%%

\begin{frame}[c]
\centering
\Huge Moving Windows
\end{frame}

\begin{frame}{The Idea}
Instead of training once, \alert{retrain the model each month} (or each quarter) using data up to that point.  Predict only the next period.

\vspace{0.3cm}
\begin{barenumerate}
\item Train on months 1 through $t-1$.
\item Predict month $t$.
\item Record the predictions.
\item Move forward: train on months 1 through $t$, predict month $t+1$.
\item Repeat until the end of the sample.
\end{barenumerate}

\vspace{0.3cm}
This produces a time series of \alert{out-of-sample predictions}, each made without any future information.
\end{frame}

\begin{frame}{Expanding vs.\ Rolling Windows}
\begin{baritemize}
\item \textbf{Expanding window:} the training set always starts at the beginning and grows over time.  More data $=$ more stable estimates.
\item \textbf{Rolling window:} the training set is a fixed-length window (e.g., 5 years) that slides forward.  Adapts faster to changing markets.
\end{baritemize}

\vspace{0.3cm}
\begin{center}
\begin{tabular}{lll}
\textbf{Month} & \textbf{Expanding} & \textbf{Rolling (5 yr)} \\
\hline
Predict 2020-01 & Train 2011--2019 & Train 2015--2019 \\
Predict 2021-01 & Train 2011--2020 & Train 2016--2020 \\
Predict 2022-01 & Train 2011--2021 & Train 2017--2021 \\
\end{tabular}
\end{center}
\end{frame}

\begin{frame}{Predicting One Period Ahead}
\textbf{Key principle:} at each step, we predict \alert{only the next month}.

\vspace{0.3cm}
\begin{itemize}
\item We are not forecasting 6 or 12 months ahead.
\item Each month, we rank stocks based on the model's prediction for that month's return.
\item We then observe the actual returns and record the results.
\item This simulates what we would do in real time: retrain, predict, trade, repeat.
\end{itemize}

\vspace{0.3cm}
The collection of one-step-ahead predictions across all months forms the \alert{backtest}.
\end{frame}

\begin{frame}[fragile]{Moving Window in Practice}
\alert{Prompt:} ``Using an expanding window starting with 5 years of training data, retrain a LightGBM model each month and predict return ranks one month ahead.  Collect all out-of-sample predictions.''

\vspace{0.3cm}
\small
\begin{verbatim}
  months = sorted(df['month'].unique())
  predictions = []
  for t in range(60, len(months)):
      train = df[df['month'] < months[t]]
      test  = df[df['month'] == months[t]]
      model.fit(train[features], train[target])
      pred  = model.predict(test[features])
      predictions.append(...)
\end{verbatim}
\end{frame}

%%% PART 3: SPEARMAN RANK CORRELATION %%%

\begin{frame}[c]
\centering
\Huge Spearman Rank Correlation
\end{frame}

\begin{frame}{Measuring Prediction Quality}
We predict return \alert{ranks}, not dollar returns.  So the question is: do our predicted ranks agree with the actual ranks?

\vspace{0.3cm}
\begin{baritemize}
\item We don't need the predicted values to be accurate in magnitude.
\item We need the \alert{ordering} to be right: stocks we predict to be at the top should actually outperform.
\item This calls for a \alert{rank-based} metric.
\end{baritemize}
\end{frame}

\begin{frame}{Spearman Rank Correlation}
The \alert{Spearman correlation} measures the agreement between two rankings.

\vspace{0.3cm}
\begin{baritemize}
\item Rank the predicted values 1 through $n$.
\item Rank the actual returns 1 through $n$.
\item Compute the Pearson correlation of the two rank vectors.
\end{baritemize}

\vspace{0.3cm}
\[
\rho_S = \text{corr}\bigl(\text{rank}(\hat y), \;\text{rank}(y)\bigr)
\]

\begin{itemize}
\item $\rho_S = 1$: perfect agreement in rankings.
\item $\rho_S = 0$: predictions are no better than random.
\item $\rho_S < 0$: predictions are inversely related to actual outcomes.
\end{itemize}
\end{frame}

\begin{frame}{Why Spearman?}
\begin{baritemize}
\item Directly measures what we care about: can the model \alert{rank stocks correctly}?
\item Consistent with using ranked returns as the target.
\item Insensitive to outliers --- a stock that doubles or crashes doesn't distort the metric.
\item Does not require forming portfolios or choosing portfolio weights.
\end{baritemize}

\vspace{0.3cm}
We will use Spearman as the \alert{cross-validation metric} for selecting hyperparameters: the hyperparameter setting that produces the highest Spearman on validation data is the one we choose.
\end{frame}

%%% PART 4: CROSS-VALIDATION IN THE MOVING WINDOW %%%

\begin{frame}[c]
\centering
\Huge Cross-Validation in the Moving Window
\end{frame}

\begin{frame}{The Hyperparameter Problem}
Every model has \alert{hyperparameters} that must be set before training:

\begin{itemize}
\item \textbf{LightGBM:} number of trees, learning rate, max depth, min samples per leaf
\item \textbf{Ridge:} regularization strength $\lambda$
\item \textbf{Neural net:} number of layers, hidden units, learning rate, dropout rate
\item \textbf{Random Fourier features:} number of features $D$, bandwidth $\gamma$
\end{itemize}

\vspace{0.3cm}
How do we choose them without using future data?
\end{frame}

\begin{frame}{Cross-Validation Inside the Training Window}
At each step of the moving window, \alert{before} training the final model:

\vspace{0.3cm}
\begin{barenumerate}
\item Split the training window into $K$ time-ordered folds.
\item For each candidate hyperparameter setting:
\begin{enumerate}
\item For each fold $k$: train on folds before $k$, predict fold $k$.
\item Compute the Spearman correlation on each fold's predictions.
\item Average across folds.
\end{enumerate}
\item Select the hyperparameters with the highest average Spearman.
\item Retrain on the full training window with those hyperparameters.
\end{barenumerate}
\end{frame}

\begin{frame}{Time-Series Cross-Validation}
Standard $K$-fold CV assigns folds randomly.  For time series, folds must respect the time order.

\vspace{0.3cm}
\textbf{Example with 5 folds} (training window = 2011--2019):

\begin{center}
\small
\begin{tabular}{lll}
\textbf{Fold} & \textbf{Train} & \textbf{Validate} \\
\hline
1 & 2011--2014 & 2015 \\
2 & 2011--2015 & 2016 \\
3 & 2011--2016 & 2017 \\
4 & 2011--2017 & 2018 \\
5 & 2011--2018 & 2019 \\
\end{tabular}
\end{center}

\vspace{0.3cm}
Each validation fold comes \alert{after} its training fold --- no look-ahead bias.
\end{frame}

\begin{frame}{Spearman in Each Validation Fold}
In each validation fold, we have predictions and actual returns for all stocks in that fold's months.

\vspace{0.3cm}
\begin{baritemize}
\item Compute the Spearman correlation between predicted and actual return ranks \alert{within each month} of the fold.
\item Average the monthly Spearman correlations within the fold.
\item Then average across all $K$ folds.
\end{baritemize}

\vspace{0.3cm}
This gives a single number summarizing how well a given hyperparameter setting ranks stocks on held-out future data --- exactly what we need.
\end{frame}

\begin{frame}{The Complete Loop}
\begin{barenumerate}
\item For each prediction month $t$:
\begin{enumerate}
\item Define the training window (months 1 through $t{-}1$).
\item Run time-series CV within the training window, selecting hyperparameters by Spearman.
\item Train the model on the full training window with the best hyperparameters.
\item Predict month $t$.
\end{enumerate}
\item Collect all one-step-ahead predictions.
\end{barenumerate}

\vspace{0.3cm}
We now have out-of-sample predicted ranks for every stock in every month of the test period.  In subsequent sessions, we will use these predictions to form and evaluate portfolios.
\end{frame}

\begin{frame}{Computational Cost}
This loop is expensive: at each step, we run $K$-fold CV (training the model $K$ times per hyperparameter setting) and then train once more on the full window.

\vspace{0.3cm}
\textbf{Practical shortcuts:}
\begin{baritemize}
\item Retrain quarterly instead of monthly (predict 3 months with the same model).
\item Use a coarse hyperparameter grid.
\item Fix hyperparameters after an initial CV and only retrain the model weights in subsequent months.
\item Use fast models (LightGBM, ridge) for the full loop; reserve slow models (neural nets) for fewer retraining dates.
\end{baritemize}
\end{frame}

\end{document}
